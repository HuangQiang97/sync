* 有限相位移相器：交叉熵算法，遗传算法、多标签分类。

* 用户间干扰：注意力机制，seq2seq(transformer)。

* 端到端学习：编码作为网路层+Geotorch约束。


* 用户移动：注意力机制。

* 多用户或者宽带模型，利用环境的IID可以重用模型或者迁移学习。

* 稀疏网络：高层次到低层次：多任务和多模态学习->由小单元组成的稀疏运算通路（如CapsuleNet）->注意力机制(让神经网络把注意力集中在稀疏的信息上)->通道、权重的稀疏连接（认为神经网络的微观结构是一堆Subnetwork，比如Relu本身其实就意味着产生稀疏性，比如神经网络train完了出现的无数零权重和空连接，几乎全为零的特征图）。

  但是受限于基础设施，没有利用上这些稀疏的连接，只有在进行网络压缩的时候，才会利用一下稀疏性。如果利用上权重的稀疏性，还能进一步压缩网络。并且这还是平均过的稀疏性，在每一个输入数据的inference中，都会产生不一样的稀疏性，也就是每一个输入样本真正利用上的权重和需要的计算量其实是很少的，只是限于基础设施，无法利用这种微观的稀疏性。

* 多任务：损失权重；天线选择：多分类/强化学习；无监督：估计信道输入+完美信道频谱效率；模型压缩：稀疏性；模拟预编码：多分类；混合预编码：模拟+数字对齐；宽带模型：IID/迁移学习;用户间干扰/移动：注意力机制;降低复杂度：码本设计；

* 缝合：多头注意力+多层注意力+多任务（OPT+RF（码本多分类问题）/BB+RF，损失权重）+有监督/无监督（估计H+完美H）+在线监测+天线选择（多分类/强化学习）+模型压缩+宽带（迁移学习）+量化移相器+正交性约束